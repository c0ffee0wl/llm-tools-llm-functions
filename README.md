# llm-tools-llm-functions

[![PyPI](https://img.shields.io/pypi/v/llm-tools-llm-functions.svg)](https://pypi.org/project/llm-tools-llm-functions/)
[![Changelog](https://img.shields.io/github/v/release/c0ffee0wl/llm-tools-llm-functions?include_prereleases&label=changelog)](https://github.com/c0ffee0wl/llm-tools-llm-functions/releases)
[![Tests](https://github.com/c0ffee0wl/llm-tools-llm-functions/actions/workflows/test.yml/badge.svg)](https://github.com/c0ffee0wl/llm-tools-llm-functions/actions/workflows/test.yml)

Integration plugin for using [llm-functions](https://github.com/sigoden/llm-functions) tools with Simon Willison's [llm](https://llm.datasette.io/) CLI.

This plugin automatically discovers and registers llm-functions tools (written in Bash, JavaScript, or Python) as native llm tools, allowing you to use them seamlessly with function calling.

## Features

- **Automatic Discovery**: Scans your llm-functions directory and registers all available tools
- **Multi-Language Support**: Works with Bash (.sh), JavaScript (.js), and Python (.py) tools
- **Configurable**: Supports environment variables and YAML configuration
- **Security**: Tool allowlist/denylist support for fine-grained control
- **Type-Safe**: Validates parameters and provides helpful error messages
- **Zero Configuration**: Works out-of-the-box with existing llm-functions setups (detects `AICHAT_FUNCTIONS_DIR`)

## Installation

Install this plugin in the same environment as [LLM](https://llm.datasette.io/).

```bash
llm install llm-tools-llm-functions
```

Or install from source:

```bash
llm install -e /path/to/llm-tools-llm-functions
```

## Prerequisites

You need to have [llm-functions](https://github.com/sigoden/llm-functions) set up with:

1. A functions directory (default: `~/llm-functions`)
2. A `functions.json` file (auto-generated by llm-functions)
3. Tool scripts in the `tools/` subdirectory

See the [llm-functions documentation](https://github.com/sigoden/llm-functions#readme) for setup instructions.

## Usage

Once installed, all llm-functions tools are automatically available as llm tools.

### Basic Usage

```bash
# List available tools (including llm-functions tools)
llm tools list

# Use a tool in a prompt
llm "What files are in the current directory?" --tool execute_command

# Use multiple tools
llm "Analyze the system and suggest optimizations" \
    --tool execute_command \
    --tool read_file
```

### Examples with Common llm-functions Tools

```bash
# Execute a shell command
llm "Show me disk usage" --tool execute_command

# Read and analyze a file
llm "Summarize the README" --tool read_file

# Web scraping
llm "Get the latest news from example.com" --tool web_fetch

# Multiple tool workflow
llm "Clone the repo at github.com/user/project, read the README, and create a summary" \
    --tool execute_command \
    --tool read_file
```

### Interactive Mode

```bash
# Start interactive session with tools available
llm chat --tool execute_command --tool read_file

> "What files are larger than 100MB?"
> "Read the package.json file"
> "Run npm install"
```

## Configuration

### Environment Variables

The plugin supports multiple environment variables with the following precedence (highest to lowest):

- `LLM_FUNCTIONS_DIR`: Path to llm-functions directory (plugin-specific override)
- `AICHAT_FUNCTIONS_DIR`: Path to llm-functions directory (standard llm-functions variable)
- `LLM_FUNCTIONS_JSON`: Path to functions.json (default: `$LLM_FUNCTIONS_DIR/functions.json`)

**If you already have llm-functions set up**, the plugin will automatically detect `AICHAT_FUNCTIONS_DIR` and work without additional configuration.

```bash
# Option 1: Use standard llm-functions environment variable (works with AIChat)
export AICHAT_FUNCTIONS_DIR="/path/to/llm-functions"

# Option 2: Use plugin-specific override
export LLM_FUNCTIONS_DIR="/custom/path/to/llm-functions"

# Optional: Custom functions.json path
export LLM_FUNCTIONS_JSON="/custom/path/to/functions.json"
```

**Precedence example:**
```bash
# Both set - LLM_FUNCTIONS_DIR wins
export AICHAT_FUNCTIONS_DIR="/path/to/aichat-functions"
export LLM_FUNCTIONS_DIR="/path/to/llm-functions"  # This one is used
```

### Configuration File

Create `~/.config/io.datasette.llm/llm-functions.yaml`:

```yaml
# Path to llm-functions directory
functions_directory: ~/llm-functions

# Path to functions.json (optional, defaults to functions_directory/functions.json)
# functions_json: ~/custom/functions.json

# Tool allowlist (optional, if set only these tools are available)
# tool_allowlist:
#   - execute_command
#   - read_file
#   - web_fetch

# Tool denylist (optional, these tools will be blocked)
tool_denylist:
  - dangerous_tool
  - another_blocked_tool

# Enable llm-functions guard_operation.sh (default: true)
enable_guard: true

# Maximum output size in bytes (default: 1MB)
max_output_size: 1048576

# Execution timeout in seconds (default: 30)
timeout: 30
```

### Security Configuration

**Important**: Some llm-functions tools (like `execute_command`) can run arbitrary code. Use security features to limit exposure:

#### Tool Allowlist (Recommended)

Only allow specific tools:

```yaml
# ~/.config/io.datasette.llm/llm-functions.yaml
tool_allowlist:
  - read_file
  - list_directory
  - web_fetch
```

#### Tool Denylist

Block dangerous tools:

```yaml
tool_denylist:
  - execute_command
  - write_file
  - delete_file
```

## How It Works

1. **Discovery**: On startup, the plugin scans `functions.json` for tool definitions
2. **Parsing**: Tool metadata (name, description, parameters) is extracted
3. **Wrapping**: Each tool is wrapped as a Python callable function
4. **Registration**: Tools are registered with llm via the `register_tools()` hook
5. **Execution**: When called, tools execute via subprocess with proper environment setup

### Technical Details

- **Parameter Mapping**: JSON Schema parameters â†’ Python function arguments
- **Execution Environment**: Tools run with `LLM_OUTPUT` and `ROOT_DIR` environment variables
- **Output Handling**: Captures both stdout and `LLM_OUTPUT` file contents
- **Error Handling**: Proper error messages for missing scripts, failed executions, and timeouts

## Troubleshooting

### "No tools found"

- Verify llm-functions is installed: `ls ~/llm-functions`
- Check functions.json exists: `ls ~/llm-functions/functions.json`
- Ensure tools are listed: `cat ~/llm-functions/functions.json`

### "Tool script not found"

- Tool scripts must be in `tools/` subdirectory
- Scripts must have correct extension (.sh, .js, .py)
- Scripts must be executable: `chmod +x ~/llm-functions/tools/*.sh`

### "Tool execution timeout"

- Increase timeout in config:
  ```yaml
  timeout: 60  # 60 seconds
  ```

### "Permission denied"

- Make sure tool scripts are executable:
  ```bash
  chmod +x ~/llm-functions/tools/*.sh
  ```

### Debug Mode

Enable verbose output:

```bash
# Show tool discovery and registration
llm plugins

# Show available tools
llm tools list

# Debug tool execution (use llm's --tools-debug flag when available)
llm "test" --tool my_tool --tools-debug
```

## Development

To set up this plugin locally, first checkout the code. Then create a new virtual environment:

```bash
cd llm-tools-llm-functions
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```

Now install the dependencies and test dependencies:

```bash
python -m pip install -e '.[test]'
```

To run the tests:

```bash
python -m pytest
```

Run tests with coverage:

```bash
python -m pytest --cov=. --cov-report=html
```

## Architecture

The plugin consists of three main components:

### 1. FunctionJsonParser (`llm_tools_llm_functions.py`)

Parses llm-functions' `functions.json` file to extract tool definitions.

### 2. ToolExecutor (`llm_tools_llm_functions.py`)

Executes tool scripts via subprocess with proper environment setup:
- Sets `LLM_OUTPUT` to temporary file
- Sets `ROOT_DIR` to functions directory
- Handles parameter mapping to command-line arguments
- Captures output and errors

### 3. ToolWrapper (`llm_tools_llm_functions.py`)

Wraps tool definitions as Python callables compatible with llm's tool system:
- Dynamically generates function signatures
- Validates required parameters
- Creates comprehensive docstrings from tool metadata

### 4. Config (`config.py`)

Manages configuration from environment variables and YAML file:
- Path configuration
- Security settings (allowlist/denylist)
- Execution limits (timeout, output size)

## Comparison with llm-functions

| Feature | llm-functions | llm-tools-llm-functions |
|---------|---------------|------------------------|
| **CLI Tool** | `llm-functions` (AIChat) | `llm` (Simon Willison) |
| **Tool Format** | Bash/JS/Python scripts | Same (reuses llm-functions tools) |
| **Function Calling** | Built-in | Via llm's tool system |
| **Agent System** | Built-in agents | Use llm's conversation features |
| **Model Support** | Via AIChat config | Via llm (wider model support) |
| **Tool Discovery** | Automatic | Automatic (via plugin) |
| **Use Case** | Standalone AI CLI | Integrate llm-functions into llm ecosystem |

## Related Projects

- [llm](https://llm.datasette.io/) - Simon Willison's LLM CLI tool
- [llm-functions](https://github.com/sigoden/llm-functions) - Multi-language LLM tools framework
- [llm-tools-context](https://github.com/c0ffee0wl/llm-linux-setup/tree/main/llm-tools-context) - Terminal context tool for llm

## Contributing

Contributions are welcome! Please:

1. Fork the repository
2. Create a feature branch
3. Add tests for new functionality
4. Ensure all tests pass
5. Submit a pull request

## License

Apache License 2.0

## Security Considerations

**Warning**: This plugin executes external scripts that can run arbitrary code. Follow these security best practices:

1. **Review Tools**: Examine llm-functions tools before enabling them
2. **Use Allowlists**: Explicitly allow only trusted tools
3. **Limit Prompts**: Be cautious with user-provided prompts that trigger tool execution
4. **Sandbox Environment**: Consider running in containers or VMs for untrusted tools
5. **Monitor Execution**: Check logs and outputs for suspicious activity

**Prompt Injection Risk**: LLMs can be manipulated via crafted inputs to execute unintended tools. Never give tools like `execute_command` access to:
- Untrusted user input
- External/scraped content
- Data that could contain instructions

See [llm's security documentation](https://llm.datasette.io/en/stable/tools.html) for more information.

## Changelog

See [Releases](https://github.com/c0ffee0wl/llm-tools-llm-functions/releases).

## Support

- **Issues**: [GitHub Issues](https://github.com/c0ffee0wl/llm-tools-llm-functions/issues)
- **Discussions**: [GitHub Discussions](https://github.com/c0ffee0wl/llm-tools-llm-functions/discussions)
- **LLM Documentation**: [llm.datasette.io](https://llm.datasette.io/)
- **llm-functions Documentation**: [github.com/sigoden/llm-functions](https://github.com/sigoden/llm-functions)
